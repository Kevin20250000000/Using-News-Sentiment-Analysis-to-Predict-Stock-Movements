# -*- coding: utf-8 -*-
"""stock-prediction-gan-X-sentiment-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3hig-3T1TwopJ0uiKohU2w3QJMOzIYi

# Introduction

In recent years, Generative Adversarial Networks (GANs) have achieved remarkable success in generating realistic images and videos, as well as transforming data across modalities. However, their application to stock price forecasting—a complex time series problem—remains an emerging area of research. The inherent volatility of stock markets and the significant influence of investor sentiment present unique challenges for accurate prediction.

Recent studies have begun to explore the integration of sentiment analysis into GAN-based models to enhance stock price prediction. For instance, the FB-GAN model combines historical stock data with sentiment scores derived from news articles using BERT, demonstrating improved predictive performance across major equities like Amazon and Apple . Similarly, sentiment-guided adversarial learning frameworks incorporate social media sentiment data, leading to more accurate forecasts compared to traditional models such as ARIMA and LSTM .

In this notebook, I develop a model to forecast Tesla (TSLA) stock prices by integrating historical data and technical indicators with external factors like trader sentiment and brand reputation, as reflected in social media posts. This comprehensive approach aims to capture the multifaceted dynamics of the market, potentially leading to more accurate and robust stock price predictions.

# Table of contents
1. [Importing Necessary Libraries](#section-one)
1. [Get weekly sentiment for stock ticker](#section-two)
1. [Get final dataset for training](#section-three)
1. [Build GAN model](#section-four)
1. [Train and test model](#section-five)
1. [Conclusions](#section-six)

<a id="section-one"></a>
# Importing Necessary Libraries
"""

!pip install pydot graphviz

!pip install nltk

import nltk
nltk.download('vader_lexicon')

from tensorflow.keras.optimizers import Adam
import os
import numpy as np
import csv
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
import math
import time
import tensorflow as tf
from tensorflow.keras.layers import GRU, LSTM, Bidirectional, Dense, Flatten, Conv1D, BatchNormalization, LeakyReLU, Dropout
from tensorflow.keras import Sequential
from tensorflow.keras.utils import plot_model
from keras.utils import plot_model
from pickle import load
from sklearn.metrics import mean_squared_error
from tqdm import tqdm
import statsmodels.api as sm
from math import sqrt
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
from pickle import dump
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import unicodedata

import warnings
warnings.filterwarnings("ignore")

"""<a id="section-two"></a>
# Get weekly sentiment for stock ticker

Stock price movements are influenced by more than just historical data; external factors like social media sentiment can have significant impacts. Notable examples include Elon Musk's tweets, which have been shown to cause substantial fluctuations in Tesla's stock price. For instance, in August 2018, Musk tweeted about taking Tesla private at $420 per share, leading to a more than 6% increase in the stock price and prompting a temporary trading halt . Conversely, in May 2020, his tweet stating "Tesla stock price too high imo" resulted in a $14 billion drop in Tesla's market value .

Research indicates a significant relationship between Twitter sentiment and stock returns. Studies have found that peaks in Twitter activity and sentiment can lead to abnormal stock returns . This suggests that analyzing the tone of social media posts, particularly on platforms like Twitter, can provide valuable insights into market dynamics.

In this notebook, we will incorporate sentiment analysis of Twitter posts to assess the mood of stock market participants. By integrating this external indicator with traditional financial data, we aim to enhance the accuracy and robustness of stock price predictions.
"""

stock_name = 'TSLA'

all_tweets = pd.read_csv('/content/stock_tweets.csv')

print(all_tweets.shape)
all_tweets.head()

df = all_tweets[all_tweets['Stock Name'] == stock_name]
print(df.shape)
df.head()

sent_df = df.copy()
sent_df["sentiment_score"] = ''
sent_df["Negative"] = ''
sent_df["Neutral"] = ''
sent_df["Positive"] = ''
sent_df.head()

"""To obtain sentiment (polarity) scores, we utilize VADER (Valence Aware Dictionary and sEntiment Reasoner), a rule-based sentiment analysis tool designed to detect both the polarity (positive/negative) and intensity of emotions in text. VADER is particularly effective for analyzing social media content and is available through the NLTK library.


VADER operates using a lexicon that maps lexical features—including words, emoticons, acronyms, and slang—to sentiment intensity scores ranging from -4 (most negative) to +4 (most positive). For example, the word "horrible" might be assigned a score of -2.5, while "okay" could have a score of 0.9 .


The overall sentiment of a text is calculated by summing the sentiment scores of individual words and then normalizing this sum to produce a compound score between -1 (most negative) and +1 (most positive). VADER also accounts for contextual elements such as punctuation, capitalization, degree modifiers (e.g., "very," "extremely"), and conjunctions like "but," which can alter the sentiment intensity .


This approach allows VADER to effectively analyze sentiment in short, informal texts, making it well-suited for applications like Twitter sentiment analysis.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sentiment_analyzer = SentimentIntensityAnalyzer()
# for indx, row in sent_df.T.items():
#     try:
#         sentence_i = unicodedata.normalize('NFKD', sent_df.loc[indx, 'Tweet'])
#         sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)
#         sent_df.at[indx, 'sentiment_score'] = sentence_sentiment['compound']
#         sent_df.at[indx, 'Negative'] = sentence_sentiment['neg']
#         sent_df.at[indx, 'Neutral'] = sentence_sentiment['neu']
#         sent_df.at[indx, 'Positive'] = sentence_sentiment['pos']
#     except TypeError:
#         print(sent_df.loc[indx, 'Tweet'])
#         print(indx)
#         break

sent_df.head()

sent_df['Date'] = pd.to_datetime(sent_df['Date'])
sent_df['Date'] = sent_df['Date'].dt.date
sent_df = sent_df.drop(columns=['Negative', 'Positive', 'Neutral', 'Stock Name', 'Company Name'])

sent_df.head()

sent_df['sentiment_score'] = pd.to_numeric(sent_df['sentiment_score'], errors='coerce')

sent_df[['Date','sentiment_score']]

# sent_df.groupby([sentdf[['Date']]])
twitter_df = sent_df.groupby('Date').mean(numeric_only=True)
print(twitter_df.shape)

"""As the result of sentiment analysis we get average polarity scores of all tweets about a cartain stock ticker for each day:"""

twitter_df.head()

"""<a id="section-three"></a>
# Get final dataset for training
"""

all_stocks = pd.read_csv('/content/stock_yfinance_data.csv')
print(all_stocks.shape)
all_stocks.head()

stock_df = all_stocks[all_stocks['Stock Name'] == stock_name]
stock_df['Date'] = pd.to_datetime(stock_df['Date'])
stock_df['Date'] = stock_df['Date'].dt.date

final_df = stock_df.join(twitter_df, how="left", on="Date")
final_df = final_df.drop(columns=['Stock Name'])
print(final_df.shape)

final_df.head()

"""Let's plot historical price data for the analyzed stock ticker:"""

fig, ax = plt.subplots(figsize=(15,8))
ax.plot(final_df['Date'], final_df['Close'], color='#008B8B')
ax.set(xlabel="Date", ylabel="USD", title=f"{stock_name} Stock Price")
ax.xaxis.set_major_formatter(DateFormatter("%Y"))
plt.show()

"""### Adding technical indicators

To help the network understand the bigger picture of the market we add different technical indicators to the training data, such as moving averages, Bollinger bands etc., which describe the development of stock price not only for the current day, but for the past week or more.

**MA(7)** stans for Moving Average for past 7 days, whereas **MA(20)** means Moving Average for past 20 days.

**EMA** is Exponential Moving average and we can calculate it as:
* *EMA_t = Pclose + (EMA_t-1 * (100 - P))*

**Bollinger Bands** are calculated as:
* middle line: *stdev(MA(20))*
* upper bound: *MA(20) + 2stdev(MA(20))*
* lower bound: *MA(20) - 2stdev(MA(20))*
"""

def get_tech_ind(data):
    data['MA7'] = data.iloc[:,4].rolling(window=7).mean() #Close column
    data['MA20'] = data.iloc[:,4].rolling(window=20).mean() #Close Column

    data['MACD'] = data.iloc[:,4].ewm(span=26).mean() - data.iloc[:,1].ewm(span=12,adjust=False).mean()
    #This is the difference of Closing price and Opening Price

    # Create Bollinger Bands
    data['20SD'] = data.iloc[:, 4].rolling(20).std()
    data['upper_band'] = data['MA20'] + (data['20SD'] * 2)
    data['lower_band'] = data['MA20'] - (data['20SD'] * 2)

    # Create Exponential moving average
    data['EMA'] = data.iloc[:,4].ewm(com=0.5).mean()

    # Create LogMomentum
    data['logmomentum'] = np.log(data.iloc[:,4] - 1)

    return data

tech_df = get_tech_ind(final_df)
dataset = tech_df.iloc[20:,:].reset_index(drop=True)
dataset.head()

def tech_ind(dataset):
    fig,ax = plt.subplots(figsize=(15, 8), dpi = 200)
    x_ = range(3, dataset.shape[0])
    x_ = list(dataset.index)

    ax.plot(dataset['Date'], dataset['MA7'], label='Moving Average (7 days)', color='g', linestyle='--')
    ax.plot(dataset['Date'], dataset['Close'], label='Closing Price', color='#6A5ACD')
    ax.plot(dataset['Date'], dataset['MA20'], label='Moving Average (20 days)', color='r', linestyle='-.')
    ax.xaxis.set_major_formatter(DateFormatter("%Y"))
    plt.title('Technical indicators')
    plt.ylabel('Close (USD)')
    plt.xlabel("Year")
    plt.legend()

    plt.show()

"""Let's plot Moving Averages for our data:"""

tech_ind(tech_df)

dataset.iloc[:, 1:] = pd.concat([dataset.iloc[:, 1:].ffill()])

datetime_series = pd.to_datetime(dataset['Date'])
datetime_index = pd.DatetimeIndex(datetime_series.values)
dataset = dataset.set_index(datetime_index)
dataset = dataset.sort_values(by='Date')
dataset = dataset.drop(columns='Date')

def normalize_data(df, range, target_column):

    '''
    df: dataframe object
    range: type tuple -> (lower_bound, upper_bound)
        lower_bound: int
        upper_bound: int
    target_column: type str -> should reflect closing price of stock
    '''

    target_df_series = pd.DataFrame(df[target_column])
    data = pd.DataFrame(df.iloc[:, :])

    X_scaler = MinMaxScaler(feature_range=range)
    y_scaler = MinMaxScaler(feature_range=range)
    X_scaler.fit(data)
    y_scaler.fit(target_df_series)

    X_scale_dataset = X_scaler.fit_transform(data)
    y_scale_dataset = y_scaler.fit_transform(target_df_series)

    dump(X_scaler, open('X_scaler.pkl', 'wb'))
    dump(y_scaler, open('y_scaler.pkl', 'wb'))

    return (X_scale_dataset,y_scale_dataset)

def batch_data(x_data,y_data, batch_size, predict_period):
    X_batched, y_batched, yc = list(), list(), list()

    for i in range(0,len(x_data),1):
        x_value = x_data[i: i + batch_size][:, :]
        y_value = y_data[i + batch_size: i + batch_size + predict_period][:, 0]
        yc_value = y_data[i: i + batch_size][:, :]
        if len(x_value) == batch_size and len(y_value) == predict_period:
            X_batched.append(x_value)
            y_batched.append(y_value)
            yc.append(yc_value)

    return np.array(X_batched), np.array(y_batched), np.array(yc)

def split_train_test(data):
    train_size = len(data) - 20
    data_train = data[0:train_size]
    data_test = data[train_size:]
    return data_train, data_test

def predict_index(dataset, X_train, batch_size, prediction_period):

    # get the predict data (remove the in_steps days)
    train_predict_index = dataset.iloc[batch_size: X_train.shape[0] + batch_size + prediction_period, :].index
    test_predict_index = dataset.iloc[X_train.shape[0] + batch_size:, :].index

    return train_predict_index, test_predict_index

X_scale_dataset,y_scale_dataset = normalize_data(dataset, (-1,1), "Close")
X_batched, y_batched, yc = batch_data(X_scale_dataset, y_scale_dataset, batch_size = 5, predict_period = 1)
print("X shape:", X_batched.shape)
print("y shape:", y_batched.shape)
print("yc shape:", yc.shape)

X_train, X_test, = split_train_test(X_batched)
y_train, y_test, = split_train_test(y_batched)
yc_train, yc_test, = split_train_test(yc)
index_train, index_test, = predict_index(dataset, X_train, 5, 1)

input_dim = X_train.shape[1]
feature_size = X_train.shape[2]
output_dim = y_train.shape[1]

"""<a id="section-four"></a>
# Build GAN model

In this notebook we build a GAN model architecture, where the generator has 5 LSTM blocks and the discriminator has 5 convolutional and 3 dense layers with sigmoid activation function.

Generator model sructure looks like this:
![Generator model](attachment:8adaa537-fcb3-4479-a5a8-a4c9d3cc9d81.png)
"""

def make_generator_model(input_dim, output_dim, feature_size):
    model = tf.keras.Sequential([LSTM(units = 1024, return_sequences = True,
                                    input_shape=(input_dim, feature_size),recurrent_dropout = 0.3),
                               LSTM(units = 512, return_sequences = True, recurrent_dropout = 0.3),
                               LSTM(units = 256, return_sequences = True, recurrent_dropout = 0.3),
                               LSTM(units = 128, return_sequences = True, recurrent_dropout = 0.3),
                               LSTM(units = 64, recurrent_dropout = 0.3),
                               Dense(32),
                               Dense(16),
                               Dense(8),
                               Dense(units=output_dim)])
    return model

"""Discriminator model sructure looks like this:
![Discriminator model](attachment:10918abf-a634-4581-8401-15f6cfcd013e.png)
"""

def make_discriminator_model(input_dim):
    cnn_net = tf.keras.Sequential()
    cnn_net.add(Conv1D(8, input_shape=(input_dim+1, 1), kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))
    cnn_net.add(Conv1D(16, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))
    cnn_net.add(Conv1D(32, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))
    cnn_net.add(Conv1D(64, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))
    cnn_net.add(Conv1D(128, kernel_size=1, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))
    #cnn_net.add(Flatten())
    cnn_net.add(LeakyReLU())
    cnn_net.add(Dense(220, use_bias=False))
    cnn_net.add(LeakyReLU())
    cnn_net.add(Dense(220, use_bias=False, activation='relu'))
    cnn_net.add(Dense(1, activation='sigmoid'))
    return cnn_net

"""Now we define loss functions for our models. We will use BinaryCrossEntropy loss for both models:"""

def discriminator_loss(real_output, fake_output):
    loss_f = tf.keras.losses.BinaryCrossentropy(from_logits=True)
    real_loss = loss_f(tf.ones_like(real_output), real_output)
    fake_loss = loss_f(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    loss_f = tf.keras.losses.BinaryCrossentropy(from_logits=True)
    loss = loss_f(tf.ones_like(fake_output), fake_output)
    return loss

@tf.function

def train_step(real_x, real_y, yc, generator, discriminator, g_optimizer, d_optimizer):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_data = generator(real_x, training=True)
        generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])
        d_fake_input = tf.concat([tf.cast(generated_data_reshape, tf.float64), yc], axis=1)
        real_y_reshape = tf.reshape(real_y, [real_y.shape[0], real_y.shape[1], 1])
        d_real_input = tf.concat([real_y_reshape, yc], axis=1)

        real_output = discriminator(d_real_input, training=True)
        fake_output = discriminator(d_fake_input, training=True)

        g_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    g_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    d_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return real_y, generated_data, {'d_loss': disc_loss, 'g_loss': g_loss}

def train(real_x, real_y, yc, Epochs, generator, discriminator, g_optimizer, d_optimizer, checkpoint = 50):
    train_info = {}
    train_info["discriminator_loss"] = []
    train_info["generator_loss"] = []

    for epoch in tqdm(range(Epochs)):
        real_price, fake_price, loss = train_step(real_x, real_y, yc, generator, discriminator, g_optimizer, d_optimizer)
        G_losses = []
        D_losses = []
        Real_price = []
        Predicted_price = []
        D_losses.append(loss['d_loss'].numpy())
        G_losses.append(loss['g_loss'].numpy())
        Predicted_price.append(fake_price.numpy())
        Real_price.append(real_price.numpy())

        #Save model every X checkpoints
        if (epoch + 1) % checkpoint == 0:
            tf.keras.models.save_model(generator, f'./models_gan/{stock_name}/generator_V_%d.h5' % epoch)
            tf.keras.models.save_model(discriminator, f'./models_gan/{stock_name}/discriminator_V_%d.h5' % epoch)
            print('epoch', epoch + 1, 'discriminator_loss', loss['d_loss'].numpy(), 'generator_loss', loss['g_loss'].numpy())

        train_info["discriminator_loss"].append(D_losses)
        train_info["generator_loss"].append(G_losses)

    Predicted_price = np.array(Predicted_price)
    Predicted_price = Predicted_price.reshape(Predicted_price.shape[1], Predicted_price.shape[2])
    Real_price = np.array(Real_price)
    Real_price = Real_price.reshape(Real_price.shape[1], Real_price.shape[2])

    plt.subplot(2,1,1)
    plt.plot(train_info["discriminator_loss"], label='Disc_loss', color='#000000')
    plt.xlabel('Epoch')
    plt.ylabel('Discriminator Loss')
    plt.legend()

    plt.subplot(2,1,2)
    plt.plot(train_info["generator_loss"], label='Gen_loss', color='#000000')
    plt.xlabel('Epoch')
    plt.ylabel('Generator Loss')
    plt.legend()

    plt.show()

    return Predicted_price, Real_price, np.sqrt(mean_squared_error(Real_price, Predicted_price)) / np.mean(Real_price)

def plot_results(Real_price, Predicted_price, index_train):
    X_scaler = load(open('/content/X_scaler.pkl', 'rb'))
    y_scaler = load(open('/content/y_scaler.pkl', 'rb'))
    train_predict_index = index_train

    rescaled_Real_price = y_scaler.inverse_transform(Real_price)
    rescaled_Predicted_price = y_scaler.inverse_transform(Predicted_price)

    predict_result = pd.DataFrame()
    for i in range(rescaled_Predicted_price.shape[0]):
        y_predict = pd.DataFrame(rescaled_Predicted_price[i], columns=["predicted_price"], index=train_predict_index[i:i+output_dim])
        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)

    real_price = pd.DataFrame()
    for i in range(rescaled_Real_price.shape[0]):
        y_train = pd.DataFrame(rescaled_Real_price[i], columns=["real_price"], index=train_predict_index[i:i+output_dim])
        real_price = pd.concat([real_price, y_train], axis=1, sort=False)

    predict_result['predicted_mean'] = predict_result.mean(axis=1)
    real_price['real_mean'] = real_price.mean(axis=1)

    plt.figure(figsize=(16, 8))
    plt.plot(real_price["real_mean"])
    plt.plot(predict_result["predicted_mean"], color = 'r')
    plt.xlabel("Date")
    plt.ylabel("Stock price")
    plt.legend(("Real price", "Predicted price"), loc="upper left", fontsize=16)
    plt.title("The result of Training", fontsize=20)
    plt.show()

    predicted = predict_result["predicted_mean"]
    real = real_price["real_mean"]
    For_MSE = pd.concat([predicted, real], axis = 1)
    RMSE = np.sqrt(mean_squared_error(predicted, real))
    print('-- Train RMSE -- ', RMSE)

## Test Code

@tf.function

def eval_op(generator, real_x):
    generated_data = generator(real_x, training = False)

    return generated_data

def plot_test_data(Real_test_price, Predicted_test_price, index_test):
    X_scaler = load(open('X_scaler.pkl', 'rb'))
    y_scaler = load(open('y_scaler.pkl', 'rb'))
    test_predict_index = index_test

    rescaled_Real_price = y_scaler.inverse_transform(Real_test_price)
    rescaled_Predicted_price = y_scaler.inverse_transform(Predicted_test_price)

    predict_result = pd.DataFrame()
    for i in range(rescaled_Predicted_price.shape[0]):
        y_predict = pd.DataFrame(rescaled_Predicted_price[i], columns=["predicted_price"], index=test_predict_index[i:i+output_dim])
        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)

    real_price = pd.DataFrame()
    for i in range(rescaled_Real_price.shape[0]):
        y_train = pd.DataFrame(rescaled_Real_price[i], columns=["real_price"], index=test_predict_index[i:i+output_dim])
        real_price = pd.concat([real_price, y_train], axis=1, sort=False)

    predict_result['predicted_mean'] = predict_result.mean(axis=1)
    real_price['real_mean'] = real_price.mean(axis=1)

    predicted = predict_result["predicted_mean"]
    real = real_price["real_mean"]
    For_MSE = pd.concat([predicted, real], axis = 1)
    RMSE = np.sqrt(mean_squared_error(predicted, real))
    print('Test RMSE: ', RMSE)

    plt.figure(figsize=(16, 8))
    plt.plot(real_price["real_mean"], color='#00008B')
    plt.plot(predict_result["predicted_mean"], color = '#8B0000', linestyle='--')
    plt.xlabel("Date")
    plt.ylabel("Stock price")
    plt.legend(("Real price", "Predicted price"), loc="upper left", fontsize=16)
    plt.title(f"Prediction on test data for {stock_name}", fontsize=20)
    plt.show()

learning_rate = 5e-4
epochs = 500

g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

generator = make_generator_model(X_train.shape[1], output_dim, X_train.shape[2])
discriminator = make_discriminator_model(X_train.shape[1])

plot_model(generator, to_file='generator_keras_model.png', show_shapes=True)

tf.keras.utils.plot_model(discriminator, to_file='discriminator_keras_model.png', show_shapes=True)

"""<a id="section-five"></a>
# Train and test model

Train model on train data and save model weights for every 50 epochs:
"""

predicted_price, real_price, RMSPE = train(X_train, y_train, yc_train, epochs, generator, discriminator, g_optimizer, d_optimizer)

test_generator = tf.keras.models.load_model(f'./models_gan/{stock_name}/generator_V_{epochs-1}.h5')

"""Predict and plot results for test data:"""

predicted_test_data = eval_op(test_generator, X_test)
plot_test_data(y_test, predicted_test_data,index_test)

"""<a id="section-six"></a>
# Conclusions

Generative Adversarial Networks (GANs) have shown promise in modeling time series data like stock prices, particularly when combined with technical indicators and sentiment analysis from platforms such as Twitter. For widely followed stocks like Tesla, the abundance of tweet volumes provides rich sentiment data, enhancing prediction accuracy.

However, for less-discussed stocks, limited social media activity can result in sparse or noisy sentiment inputs, potentially diminishing model performance. A study by Anjaneyulu et al. (2024) highlights that low-popularity stocks present challenges due to restricted data availability and increased volatility. The research suggests that while GANs can improve predictions for high-volume tickers, their effectiveness may be constrained for stocks with limited sentiment data .
eudoxuspress.com

Therefore, while GANs are promising for stock forecasting, their efficacy is influenced by the availability and quality of sentiment data. In scenarios with limited social media activity, alternative approaches or enhanced data augmentation techniques may be necessary to maintain predictive performance.
"""